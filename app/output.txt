O que você precisa conhecer Conhecimento em Python e Spark; Atuação na evolução dos processos de ingestão e integração de dados; Conhecimento com arquitetura de Big Data (AWS);Conhecimento na construção de reports em Power BI e Quicksight (desejável).O que pode alavancar sua aplicação? Conhecimento com Athena;Conhecimento em RedShift;Conhecimentos em Glue; Conhecimento em EMR.

- Google Cloud Platform (BigQuery, Storage, Composer, Pub/Sub, Dataproc, Functions);- Python no contexto de dados (leitura de arquivos, APIs, transformações e ingestões de dados);- Banco de dados relacionais.

Experiência comprovada em engenharia de dados e processamento de dados em ambientes de nuvem, preferencialmente utilizando AWS e Databricks.Habilidades sólidas de programação em Python/PySpark e SQL para desenvolvimento de pipelines de dados.Conhecimento das tecnologias AWS, como S3, Redshift, Glue Data Catalog, AWS Cloud Watch, Amazon Simple Notification Service, AWS Cloud Trail, IAM, entre outras.Conhecimento em Informatica CloudExperiência em otimização de desempenho de consultas e processamento distribuído.Familiaridade com metodologias de desenvolvimento ágil e ferramentas de controle de versão.Fortes habilidades de resolução de problemas e capacidade de trabalhar de forma independente.Excelentes habilidades de comunicação verbal e escrita para colaborar com equipes multifuncionais e explicar soluções técnicas de maneira clara.Modelagem de Dados (Modelagem para BI/DataMart - Start Schema/Snowflake, Modelagem de dados para NoSQL).



O que a Cadastra espera de você:Experiência em projetos de Data Lake com Google Cloud (GCP);Domínio de Python e SQL no desenvolvimento de soluções para Engenharia de Dados;ETL/ELT: conhecimentos em extração, sanitização, deduplicação, unificação, agregação e anonimização de dados;Experiência com ferramentas de orquestração de fluxo de dados, preferencialmente Airflow;Experiência em extração de dados via API's;Experiência em ferramenta de versionamento de código, preferencialmente Github;Capacidade analítica na resolução de problemas;Conhecimento da teoria de Banco de Dados, Data Lake, Data Warehouse;DiferencialConhecimento na criação de pipelines em CI/CD, GitHub Actions / Cloud Build.Espanhol ou inglês intermediário.

O que buscamos em você: Expertise em Git e linha de comando;Gerenciamento de serviços de dados em cloud;Conhecimento em Airflow ou outra ferramenta de orquestração de dados;Habilidades em Deploy e Clusteres Kubernetes;Vivência em contêineres Docker;Conhecimento em Python, SQL e Databricks/Spark;Habilidade com arquitetura de dados;Capacidade de auxiliar a utilização de code reviews;Criação de analytics das métricas do time Comunicação clara em tópicos complexos e técnicos;Boa interlocução com os stakeholders.

Conhecimento avançado da AWS: você deve ser capaz de usar e propor arquiteturas utilizando uma ampla variedade de serviços da AWS, como EC2, S3, Redshift, EMR, Kinesis, Glue, entre outros;Habilidade em linguagens de programação: você deve ter proficiência em uma ou mais linguagens de programação, como Python, Spark, entre outras;Habilidade em banco de dados e armazenamento de dados: você deve ser capaz de trabalhar com bancos de dados SQL e NoSQL, como MySQL, PostgreSQL, MongoDB, Cassandra, DynamoDB, entre outros;Proficiência em programação de Bancos de dados Oracle e SQLserver , partições, indices, principalmente subqueries , cursores e procedures.;Conhecimento de Big Data e ferramentas de análise: você deve ser capaz de trabalhar com ferramentas como Hadoop, Spark, Hive, Presto, Sqoop, entre outras; Habilidade em desenvolvimento de ETL: você deve ser capaz de desenvolver pipelines de dados escaláveis e eficientes usando ferramentas como Apache NiFi, AWS Glue, AWS EMR entre outras;Conhecimento de segurança de dados e conformidade: você deve estar ciente das regulamentações de privacidade de dados, tais como GDPR e LGPD, e saber como aplicar as melhores práticas de segurança da informação na AWS;Habilidade em monitoramento e solução de problemas: você deve ser capaz de identificar problemas em pipelines de dados e sistemas de armazenamento, e ser capaz de implementar soluções para melhorar o desempenho e a disponibilidade; Habilidade em DevOps: você deve ser capaz de trabalhar com ferramentas de gerenciamento de configuração, como Ansible, Terraform, AWS CloudFormation, e ter habilidades em gerenciamento de infraestrutura como código (IaC);Promotor do Well archictect ;Conhecimento em tunning em Spark ;Conhecimento em Apache Beam , Apache Airflow;Conhecimento em Google Dataflow ;

O que precisamos que você tenha (requisitos):Disponibilidade para trabalho remoto;Vontade de aprender coisas novas todos os dias;Experiência com AWS (S3, Lambdas, EMR, IAM,...);Sólida experiência com Python, Pyspark e Airflow;Domínio em SQL;Vamos ficar felizes se você tiver (diferencial):Certificação AWS;Experiência com Kubernetes;Experiência com Delta Lake;Certificação em Airflow.

Ser altamente competente no uso de sistemas desenvolvimento Big Data para criação de aplicações que utilizam a tecnologia Hive.Capacidade de trabalhar em estreita colaboração com os Product Owners (POs) de negócio para entender os requisitos e traduzi-los em aplicações Big Data.Conhecimentos avançados em SQL, Shell, GIT e esteira Devops.Experiência em ingestão, tratamento, higienização, transformação e preparação de dados estruturados e não estruturados.

O que a Cadastra espera de você:Experiência em projetos de Data Lake com Google Cloud (GCP);Domínio de Python e SQL no desenvolvimento de soluções para Engenharia de Dados;ETL/ELT: conhecimentos em extração, sanitização, deduplicação, unificação, agregação e anonimização de dados;Experiência com ferramentas de orquestração de fluxo de dados, preferencialmente Airflow;Experiência em extração de dados via API's;Experiência em ferramenta de versionamento de código, preferencialmente Github;Capacidade analítica na resolução de problemas;Conhecimento da teoria de Banco de Dados, Data Lake, Data Warehouse;DiferencialConhecimento na criação de pipelines em CI/CD, GitHub Actions / Cloud Build.Espanhol ou inglês intermediário.

